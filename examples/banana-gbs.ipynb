{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 32-dim *Banana* example in the [GBS paper](http://proceedings.mlr.press/v118/jia20a.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<threadpoolctl.threadpool_limits at 0x2aaaaeeab2e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bayesfast as bf\n",
    "import numpy as np\n",
    "from threadpoolctl import threadpool_limits\n",
    "threadpool_limits(1) # TODO: implement a bayesfast global thread controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import special_ortho_group\n",
    "\n",
    "D = 32 # number of dims\n",
    "Q = 0.01\n",
    "lower = np.full(D, -15.) # lower bound of the prior\n",
    "upper = np.full(D, 15.) # upper bound of the prior\n",
    "bound = np.array((lower, upper)).T\n",
    "diff = bound[:, 1] - bound[:, 0]\n",
    "const = np.sum(np.log(diff)) # normalization of the flat prior\n",
    "\n",
    "np.random.seed(0)\n",
    "A = special_ortho_group.rvs(D) # random rotation of the bananas\n",
    "\n",
    "def logp(x):\n",
    "    x = x @ A.T\n",
    "    return -np.sum((x[..., ::2]**2 - x[..., 1::2])**2 / Q + \n",
    "                   (x[..., ::2] - 1)**2, axis=-1) - const\n",
    "\n",
    "def grad(x):\n",
    "    x = x @ A.T\n",
    "    _pfpx2i1 = 2 * (x[..., 1::2] - x[..., ::2]**2) / Q\n",
    "    _pfpx2i = 2 * (x[..., ::2] - 1) - 2 * x[..., ::2] * _pfpx2i1\n",
    "    res = np.empty_like(x)\n",
    "    res[..., ::2] = _pfpx2i\n",
    "    res[..., 1::2] = _pfpx2i1\n",
    "    return -res @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf.utils.random.set_generator(32) # set up the global random number generator\n",
    "bf.utils.parallel.set_backend(8) # set up the global parallel backend\n",
    "den = bf.DensityLite(logp=logp, grad=grad, input_size=D, input_scales=bound,\n",
    "                     hard_bounds=True)\n",
    "sample_trace = {'n_chain': 8, 'n_iter': 2500, 'n_warmup': 1000}\n",
    "rec = bf.Recipe(density=den, sample={'sample_trace': sample_trace},\n",
    "                post={'evidence_method': 'GBS'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:183: RuntimeWarning:  CHAIN #1 : divide by zero encountered in log\n",
      "  return np.sum(np.log(np.abs(self.to_original_grad(x_trans))),\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #1 : invalid value encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:183: RuntimeWarning:  CHAIN #2 : divide by zero encountered in log\n",
      "  return np.sum(np.log(np.abs(self.to_original_grad(x_trans))),\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #1 : divide by zero encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #2 : invalid value encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:183: RuntimeWarning:  CHAIN #3 : divide by zero encountered in log\n",
      "  return np.sum(np.log(np.abs(self.to_original_grad(x_trans))),\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #2 : divide by zero encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #3 : invalid value encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:183: RuntimeWarning:  CHAIN #4 : divide by zero encountered in log\n",
      "  return np.sum(np.log(np.abs(self.to_original_grad(x_trans))),\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #3 : divide by zero encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #4 : invalid value encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:183: RuntimeWarning:  CHAIN #5 : divide by zero encountered in log\n",
      "  return np.sum(np.log(np.abs(self.to_original_grad(x_trans))),\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #5 : invalid value encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:183: RuntimeWarning:  CHAIN #6 : divide by zero encountered in log\n",
      "  return np.sum(np.log(np.abs(self.to_original_grad(x_trans))),\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/samplers/nuts.py:140: RuntimeWarning:  CHAIN #3 : overflow encountered in exp\n",
      "  p_accept = min(1, np.exp(-energy_change))\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #5 : divide by zero encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #6 : invalid value encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/samplers/nuts.py:140: RuntimeWarning:  CHAIN #1 : overflow encountered in exp\n",
      "  p_accept = min(1, np.exp(-energy_change))\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #6 : divide by zero encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:183: RuntimeWarning:  CHAIN #7 : divide by zero encountered in log\n",
      "  return np.sum(np.log(np.abs(self.to_original_grad(x_trans))),\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #7 : invalid value encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/samplers/nuts.py:140: RuntimeWarning:  CHAIN #2 : overflow encountered in exp\n",
      "  p_accept = min(1, np.exp(-energy_change))\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/core/density.py:936: RuntimeWarning:  CHAIN #7 : divide by zero encountered in true_divide\n",
      "  _grad += self.to_original_grad2(x) / _tog\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/samplers/nuts.py:140: RuntimeWarning:  CHAIN #5 : overflow encountered in exp\n",
      "  p_accept = min(1, np.exp(-energy_change))\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/samplers/nuts.py:140: RuntimeWarning:  CHAIN #4 : overflow encountered in exp\n",
      "  p_accept = min(1, np.exp(-energy_change))\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/samplers/nuts.py:140: RuntimeWarning:  CHAIN #7 : overflow encountered in exp\n",
      "  p_accept = min(1, np.exp(-energy_change))\n",
      "/global/u2/h/hejia/bayesfast/bayesfast/samplers/nuts.py:140: RuntimeWarning:  CHAIN #6 : overflow encountered in exp\n",
      "  p_accept = min(1, np.exp(-energy_change))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CHAIN #1 : sampling proceeding [ 500 / 2500 ], last 500 samples used 41.88 seconds, while divergence encountered in 36 sample(s). (warmup)\n",
      " CHAIN #2 : sampling proceeding [ 500 / 2500 ], last 500 samples used 46.56 seconds. (warmup)\n",
      " CHAIN #3 : sampling proceeding [ 500 / 2500 ], last 500 samples used 53.21 seconds, while divergence encountered in 42 sample(s). (warmup)\n",
      " CHAIN #0 : sampling proceeding [ 500 / 2500 ], last 500 samples used 53.52 seconds. (warmup)\n",
      " CHAIN #6 : sampling proceeding [ 500 / 2500 ], last 500 samples used 53.86 seconds, while divergence encountered in 56 sample(s). (warmup)\n",
      " CHAIN #5 : sampling proceeding [ 500 / 2500 ], last 500 samples used 54.81 seconds. (warmup)\n",
      " CHAIN #4 : sampling proceeding [ 500 / 2500 ], last 500 samples used 55.66 seconds, while divergence encountered in 35 sample(s). (warmup)\n",
      " CHAIN #7 : sampling proceeding [ 500 / 2500 ], last 500 samples used 64.99 seconds, while divergence encountered in 34 sample(s). (warmup)\n",
      " CHAIN #1 : sampling proceeding [ 1000 / 2500 ], last 500 samples used 45.57 seconds. (warmup)\n",
      " CHAIN #2 : sampling proceeding [ 1000 / 2500 ], last 500 samples used 50.23 seconds. (warmup)\n",
      " CHAIN #3 : sampling proceeding [ 1000 / 2500 ], last 500 samples used 44.36 seconds. (warmup)\n",
      " CHAIN #6 : sampling proceeding [ 1000 / 2500 ], last 500 samples used 45.57 seconds. (warmup)\n",
      " CHAIN #0 : sampling proceeding [ 1000 / 2500 ], last 500 samples used 46.68 seconds. (warmup)\n",
      " CHAIN #5 : sampling proceeding [ 1000 / 2500 ], last 500 samples used 46.20 seconds. (warmup)\n",
      " CHAIN #4 : sampling proceeding [ 1000 / 2500 ], last 500 samples used 46.31 seconds. (warmup)\n",
      " CHAIN #7 : sampling proceeding [ 1000 / 2500 ], last 500 samples used 45.64 seconds. (warmup)\n",
      " CHAIN #1 : sampling proceeding [ 1500 / 2500 ], last 500 samples used 44.57 seconds.\n",
      " CHAIN #3 : sampling proceeding [ 1500 / 2500 ], last 500 samples used 43.78 seconds.\n",
      " CHAIN #2 : sampling proceeding [ 1500 / 2500 ], last 500 samples used 45.60 seconds.\n",
      " CHAIN #0 : sampling proceeding [ 1500 / 2500 ], last 500 samples used 43.79 seconds.\n",
      " CHAIN #6 : sampling proceeding [ 1500 / 2500 ], last 500 samples used 45.06 seconds.\n",
      " CHAIN #5 : sampling proceeding [ 1500 / 2500 ], last 500 samples used 44.31 seconds.\n",
      " CHAIN #4 : sampling proceeding [ 1500 / 2500 ], last 500 samples used 46.51 seconds.\n",
      " CHAIN #7 : sampling proceeding [ 1500 / 2500 ], last 500 samples used 45.96 seconds.\n",
      " CHAIN #1 : sampling proceeding [ 2000 / 2500 ], last 500 samples used 44.90 seconds.\n",
      " CHAIN #3 : sampling proceeding [ 2000 / 2500 ], last 500 samples used 43.99 seconds.\n",
      " CHAIN #2 : sampling proceeding [ 2000 / 2500 ], last 500 samples used 46.51 seconds.\n",
      " CHAIN #0 : sampling proceeding [ 2000 / 2500 ], last 500 samples used 45.10 seconds.\n",
      " CHAIN #6 : sampling proceeding [ 2000 / 2500 ], last 500 samples used 45.58 seconds.\n",
      " CHAIN #5 : sampling proceeding [ 2000 / 2500 ], last 500 samples used 46.41 seconds.\n",
      " CHAIN #4 : sampling proceeding [ 2000 / 2500 ], last 500 samples used 47.27 seconds.\n",
      " CHAIN #7 : sampling proceeding [ 2000 / 2500 ], last 500 samples used 45.22 seconds.\n",
      " CHAIN #1 : sampling finished [ 2500 / 2500 ], obtained 2500 samples in 221.63 seconds.\n",
      " CHAIN #3 : sampling finished [ 2500 / 2500 ], obtained 2500 samples in 229.98 seconds.\n",
      " CHAIN #6 : sampling finished [ 2500 / 2500 ], obtained 2500 samples in 232.28 seconds.\n",
      " CHAIN #0 : sampling finished [ 2500 / 2500 ], obtained 2500 samples in 232.51 seconds.\n",
      " CHAIN #2 : sampling finished [ 2500 / 2500 ], obtained 2500 samples in 234.27 seconds.\n",
      " CHAIN #5 : sampling finished [ 2500 / 2500 ], obtained 2500 samples in 237.66 seconds.\n",
      " CHAIN #4 : sampling finished [ 2500 / 2500 ], obtained 2500 samples in 239.80 seconds.\n",
      " CHAIN #7 : sampling finished [ 2500 / 2500 ], obtained 2500 samples in 245.44 seconds.\n",
      "\n",
      " *** SampleStep proceeding: iter #0 finished. *** \n",
      "\n",
      " ***** SampleStep finished. ***** \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/h/hejia/.conda/envs/hejia@cori-2/lib/python3.6/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n",
      "/global/homes/h/hejia/.conda/envs/hejia@cori-2/lib/python3.6/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n",
      "/global/homes/h/hejia/.conda/envs/hejia@cori-2/lib/python3.6/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n",
      "/global/homes/h/hejia/.conda/envs/hejia@cori-2/lib/python3.6/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n",
      "/global/homes/h/hejia/.conda/envs/hejia@cori-2/lib/python3.6/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n",
      "/global/homes/h/hejia/.conda/envs/hejia@cori-2/lib/python3.6/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n",
      "/global/homes/h/hejia/.conda/envs/hejia@cori-2/lib/python3.6/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n",
      "/global/homes/h/hejia/.conda/envs/hejia@cori-2/lib/python3.6/site-packages/sklearn/decomposition/_fastica.py:120: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ***** PostStep finished. ***** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "rec.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('samples',\n",
       " 'weights',\n",
       " 'weights_trunc',\n",
       " 'logp',\n",
       " 'logq',\n",
       " 'logz',\n",
       " 'logz_err',\n",
       " 'x_p',\n",
       " 'x_q',\n",
       " 'logp_p',\n",
       " 'logq_q',\n",
       " 'trace_p',\n",
       " 'trace_q',\n",
       " 'n_call')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.get()._fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-127.25294056535556, 0.0523543560539916)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.get().logz, rec.get().logz_err # fiducial value: logz = -127.364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hejia@cori-2",
   "language": "python",
   "name": "hejia-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
